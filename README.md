# LocalGridHub

A simple way to find and manage context sizes for local LLMs.

### The Problem

I built this because I was getting tired of guessing the context window size for different local models I was running with local llm providers like Ollama and LM-Studio. It's a pain, especially when you're trying to set up a sliding window and need an accurate token count.

### The Solution

So, I made a Python package that scrapes the official model registries to get the correct context sizes and other useful data. It's meant to make token counting and context management way easier.

This website is the front-end for that project. It takes the cleaned-up data from the scraper and puts it in a simple UI so you can quickly search and browse all the supported models.

### How to Use

- You can search for a specific model or browse by tokenizer family.
- Click on a model family to see all the different versions.
- You can copy the model name with one click to use it in your own projects with the LocalGrid package when it releases on pip.

### Tech Stack

- Built with React and Vite.
- The data is a static JSON file generated by the backend Python package.